{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "752d275c",
   "metadata": {},
   "source": [
    "#### 1- Install or Load Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54879b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74621634",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2') # Load the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75d7224",
   "metadata": {},
   "source": [
    "#### 2 - Load and Chunk Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8def6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_chunk_text(file_path, chunk_size=250, chunk_overlap=50):\n",
    "    # Load the text file\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "    # Initialize the text splitter with desired parameters\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,  # Set the chunk size to 250 tokens\n",
    "        chunk_overlap=chunk_overlap  # Set the chunk overlap to 50 tokens\n",
    "    )\n",
    "    # Split the document into chunks\n",
    "    chunks = text_splitter.split_text(content)\n",
    "    print(f\"Total chunks created: {len(chunks)}\")\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aaaa2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and chunk the text\n",
    "file_path = \"healthy_nutrition_info.txt\"\n",
    "chunks = load_and_chunk_text(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628b787e",
   "metadata": {},
   "source": [
    "#### 3- Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4573e826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for each chunk\n",
    "embeddings = model.encode(chunks)\n",
    " \n",
    "# Create a FAISS index and add embeddings\n",
    "def create_faiss_index(embeddings):\n",
    "    embeddings = np.array(embeddings)\n",
    "    index = faiss.IndexFlatL2(embeddings.shape[1])  # Using L2 distance\n",
    "    index.add(embeddings)\n",
    "    faiss.write_index(index, 'faiss_index.index')\n",
    " \n",
    "create_faiss_index(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e40b8b0",
   "metadata": {},
   "source": [
    "#### 4- Query FAISS Index and Retrieve Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b0d33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_faiss_index(query, model, chunks, index_file='faiss_index.index', top_k=5):\n",
    "    index = faiss.read_index(index_file)  # Load the FAISS index\n",
    "    query_embedding = model.encode([query])[0] # Generate the embedding for the query\n",
    "    D, I = index.search(np.array([query_embedding]), k=top_k)  # Search for the top k closest chunks\n",
    "    results = [chunks[i] for i in I[0]]   # Retrieve the top k chunks\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683e4a1b",
   "metadata": {},
   "source": [
    "#### 5- Demo of a User Query and Top 5 Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edb2b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_chunks(query, model, chunks):\n",
    "    return query_faiss_index(query, model, chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676b31f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is healthy nutrition?\"\n",
    "top_chunks = get_similar_chunks(query, model, chunks)\n",
    "top_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a015719",
   "metadata": {},
   "source": [
    "#### 6-Use Chat GPT to Create an Answer to a User's Query Based on Top 5 Similar Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a47fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add API settings; API type; OpenAI object here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea622b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create content message based on query and top chunks\n",
    "content_message = f\"I have a query: {query}\\n\\n  Here are 5 chunks of related information:\\n\"\n",
    "\n",
    "for i, chunk in enumerate(top_chunks):\n",
    "    content_message += f\"Chunk {i+1}: {chunk}\\n\\n\"\n",
    "\n",
    "content_message += \"Please provide an answer based solely on these chunks.\"\n",
    "content_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fc8931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the messages\n",
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=(\n",
    "            \"You are a friendly, polite and helpful AI assistant. Answer the query based only and only on the provided chunks.\"\n",
    "        )\n",
    "    ),\n",
    "    HumanMessage(content= content_message)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875c2f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the model\n",
    "res = llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db317f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the response\n",
    "print(\"Final Answer:\")\n",
    "print(res.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ecig]",
   "language": "python",
   "name": "conda-env-ecig-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
